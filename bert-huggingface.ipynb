{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT tutorial using Hugging Face\n",
    "## 教學目標\n",
    "利用 Hugging Face 套件快速使用 BERT 模型來進行下游任務訓練\n",
    "- 單一句型分類任務 (single sentence text classification)\n",
    "- 成對句型分類任務 (sentence pair text classification)\n",
    "- 問答任務 (question answering)\n",
    "- 命名實體辨識 (named-entity recognition / token-level text classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單一句型分類任務\n",
    "## 下載資料\n",
    "我們使用 IMDb reviews 資料集作為範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/dean/.local/lib/python3.8/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = wget.download(url, out='./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# 指定檔案位置，並解壓縮 .gz 結尾的壓縮檔\n",
    "tar = tarfile.open('aclImdb_v1.tar.gz', 'r:gz')\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下來我們要進行資料前處理\n",
    "但首先要觀察解壓縮後的資料夾結構:\n",
    "```\n",
    "aclImdb---\n",
    "        |--train\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--test\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--imdb.vocab\n",
    "        |--imdbEr.text\n",
    "        |--README\n",
    "```\n",
    "其中train和test資料夾中分別又有neg和pos兩種資料夾\n",
    "\n",
    "我們要針對這兩個目標資料夾進行處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 pathlib 模組 (Python3.4+)\n",
    "from pathlib import Path\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        # 利用iterdir() 來列出資料夾底下的所有檔案，此功能等同於 os.path.listdir()\n",
    "#         for text_file in (split_dir/label_dir).iterdir():\n",
    "        # 若知道副檔名為.txt\n",
    "        for text_file in (split_dir/label_dir).glob(\"*.txt\"):\n",
    "            tmp_text = text_file.read_text()\n",
    "#             print(tmp_text)\n",
    "#             exit()\n",
    "            texts.append(tmp_text)\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分訓練資料，來分出 validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 設立隨機種子來控制隨機過程\n",
    "random_seed = 42\n",
    "\n",
    "# 設定要分出多少比例的 validation data\n",
    "valid_ratio = 0.2\n",
    "\n",
    "# 使用 train_test_split 來切分資料\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=valid_ratio, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizization\n",
    "- 斷字的部份以 DistilBERT (Sanh et al., 2019) 的 tokenizer 為例\n",
    "- Hugging Face 的 tokenizer 可以直接幫你自動將資料轉換成 BERT 的輸入型式 (也就是加入[CLS]和[SEP] tokens)\n",
    "- 接著由於深度學習網路需要使用 mini-batch 進行學習，我們需要先以 PyTorch dataset 來自行建立封裝資料的物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# 在 Hugging Face 套件中可使用 .from_pretrained() 的方法來導入預訓練模型\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分別將3種資料 (train/valid/test) 做 tokenization\n",
    "# truncation 代表\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
